INFO: Model name '/data/tmp/zywei/competation/pretrain_models/chinese_roberta_wwm_ext_pytorch/vocab.txt' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/data/tmp/zywei/competation/pretrain_models/chinese_roberta_wwm_ext_pytorch/vocab.txt' is a path or url to a directory containing tokenizer files.
INFO: Didn't find file /data/tmp/zywei/competation/pretrain_models/chinese_roberta_wwm_ext_pytorch/added_tokens.json. We won't load it.
INFO: Didn't find file /data/tmp/zywei/competation/pretrain_models/chinese_roberta_wwm_ext_pytorch/special_tokens_map.json. We won't load it.
INFO: Didn't find file /data/tmp/zywei/competation/pretrain_models/chinese_roberta_wwm_ext_pytorch/tokenizer_config.json. We won't load it.
INFO: loading file /data/tmp/zywei/competation/pretrain_models/chinese_roberta_wwm_ext_pytorch/vocab.txt
INFO: loading file None
INFO: loading file None
INFO: loading file None
INFO: LOOKING AT /data/tmp/zywei/competation/try_data/train.csv
INFO: *** Example ***
INFO: guid: train-1
INFO: input_ids: 101 5687 3996 4567 4970 1366 3309 833 1139 4385 5592 3811 4568 4307 1408 102 1928 4563 5592 3811 1724 5501 3187 1213 3221 679 3221 5687 3996 4567 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: label: 0 (id = 0)
INFO: *** Example ***
INFO: guid: train-2
INFO: input_ids: 101 4507 754 5131 2228 4567 2471 6629 3314 3456 4868 5307 4142 8024 2582 720 3418 3780 8043 102 5131 2228 4567 3314 3456 4868 5307 4142 4638 3780 4545 3175 3791 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: label: 1 (id = 1)
INFO: *** Example ***
INFO: guid: train-3
INFO: input_ids: 101 150 1798 7770 6117 1327 8024 3221 6858 2792 6432 4638 7770 6117 5544 8043 102 7770 6117 1327 2471 6629 5554 1139 6117 2582 720 2843 3131 3780 4545 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: label: 0 (id = 0)
INFO: self config {
  "batch_size": 32,
  "class_list": [
    "0",
    "1"
  ],
  "config_file": "/data/tmp/zywei/competation/pretrain_models/chinese_roberta_wwm_ext_pytorch/bert_config.json",
  "data_dir": "/data/tmp/zywei/competation/try_data",
  "dev_num_examples": 2000,
  "dev_split": 0.1,
  "device": "cuda",
  "device_id": 3,
  "do_lower_case": true,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "is_logging2file": true,
  "label_on_test_set": true,
  "learning_rate": 2e-05,
  "logging_dir": "/data/tmp/zywei/competation/logging/base_try_data/base_roberta_wwm_ext",
  "model_name_or_path": "/data/tmp/zywei/competation/pretrain_models/chinese_roberta_wwm_ext_pytorch/pytorch_model.bin",
  "models_name": "base_roberta_wwm_ext",
  "num_labels": 2,
  "num_train_epochs": 8,
  "pad_size": 64,
  "require_improvement": 1000,
  "requires_grad": true,
  "seed": 369,
  "task": "base_try_data",
  "test_num_examples": 2000,
  "test_split": 0.1,
  "tokenizer_file": "/data/tmp/zywei/competation/pretrain_models/chinese_roberta_wwm_ext_pytorch/vocab.txt",
  "train_num_examples": 16000,
  "warmup_proportion": 0.1,
  "weight_decay": 0.01
}

INFO: loading configuration file /data/tmp/zywei/competation/pretrain_models/chinese_roberta_wwm_ext_pytorch/bert_config.json
INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": "base_try_data",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

INFO: loading weights file /data/tmp/zywei/competation/pretrain_models/chinese_roberta_wwm_ext_pytorch/pytorch_model.bin
INFO: ***** Running training *****
INFO:   Train Num examples = 16000
INFO:   Dev Num examples = 2000
INFO:   Num Epochs = 8
INFO:   Instantaneous batch size GPU/CPU = 32
INFO:   Total optimization steps = 4000
INFO:   Train device:cuda, id:3
INFO: Epoch [1/8]
INFO: Iter:    100,  Train Loss: 0.712143,  Train Acc: 52.47%,  Val Loss: 0.667156,  Val Acc: 62.55%,  Time: 19.848432779312134 *
INFO: Iter:    200,  Train Loss: 0.475999,  Train Acc: 65.94%,  Val Loss: 0.476303,  Val Acc: 76.30%,  Time: 40.01062870025635 *
INFO: Iter:    300,  Train Loss: 0.504548,  Train Acc: 78.28%,  Val Loss: 0.429431,  Val Acc: 80.65%,  Time: 60.45813727378845 *
INFO: Iter:    400,  Train Loss: 0.467470,  Train Acc: 80.56%,  Val Loss: 0.392964,  Val Acc: 81.75%,  Time: 80.7181179523468 *
INFO: Iter:    500,  Train Loss: 0.467569,  Train Acc: 82.06%,  Val Loss: 0.355973,  Val Acc: 84.30%,  Time: 101.02497696876526 *
INFO: Epoch [2/8]
INFO: Iter:    600,  Train Loss: 0.389176,  Train Acc: 84.78%,  Val Loss: 0.349115,  Val Acc: 84.65%,  Time: 121.24384593963623 *
INFO: Iter:    700,  Train Loss: 0.205163,  Train Acc: 87.44%,  Val Loss: 0.343567,  Val Acc: 85.35%,  Time: 141.48432302474976 *
INFO: Iter:    800,  Train Loss: 0.474740,  Train Acc: 86.69%,  Val Loss: 0.342703,  Val Acc: 85.40%,  Time: 161.7442021369934 *
INFO: Iter:    900,  Train Loss: 0.245997,  Train Acc: 85.56%,  Val Loss: 0.333042,  Val Acc: 86.55%,  Time: 181.97988152503967 *
INFO: Iter:   1000,  Train Loss: 0.381238,  Train Acc: 86.00%,  Val Loss: 0.322761,  Val Acc: 86.45%,  Time: 202.22576236724854 *
INFO: Epoch [3/8]
INFO: Iter:   1100,  Train Loss: 0.125573,  Train Acc: 89.81%,  Val Loss: 0.381321,  Val Acc: 85.30%,  Time: 222.4577214717865 
INFO: Iter:   1200,  Train Loss: 0.293859,  Train Acc: 90.56%,  Val Loss: 0.332129,  Val Acc: 87.25%,  Time: 242.68883752822876 
INFO: Iter:   1300,  Train Loss: 0.242742,  Train Acc: 91.72%,  Val Loss: 0.341404,  Val Acc: 86.25%,  Time: 262.93285155296326 
INFO: Iter:   1400,  Train Loss: 0.326599,  Train Acc: 90.81%,  Val Loss: 0.337905,  Val Acc: 86.20%,  Time: 283.18233919143677 
INFO: Iter:   1500,  Train Loss: 0.218713,  Train Acc: 91.00%,  Val Loss: 0.323596,  Val Acc: 86.75%,  Time: 303.44656109809875 
INFO: Epoch [4/8]
INFO: Iter:   1600,  Train Loss: 0.096716,  Train Acc: 94.16%,  Val Loss: 0.374597,  Val Acc: 87.10%,  Time: 323.6683270931244 
INFO: Iter:   1700,  Train Loss: 0.291616,  Train Acc: 95.00%,  Val Loss: 0.389296,  Val Acc: 86.40%,  Time: 343.93044924736023 
INFO: Iter:   1800,  Train Loss: 0.267174,  Train Acc: 94.34%,  Val Loss: 0.427985,  Val Acc: 87.00%,  Time: 364.20050835609436 
INFO: Iter:   1900,  Train Loss: 0.274880,  Train Acc: 93.75%,  Val Loss: 0.386182,  Val Acc: 85.90%,  Time: 385.4981904029846 
INFO: Iter:   2000,  Train Loss: 0.461292,  Train Acc: 93.59%,  Val Loss: 0.381065,  Val Acc: 87.00%,  Time: 405.9201657772064 
INFO: Epoch [5/8]
INFO: No optimization for a long time, auto-stopping...
INFO: ***** Running testing *****
INFO:   Test Num examples = 2000
INFO: Test Loss:  0.39,  Test Acc: 85.65%
INFO: Precision, Recall and F1-Score...
INFO:               precision    recall  f1-score   support

           0     0.9115    0.7906    0.8468      1003
           1     0.8142    0.9228    0.8651       997

    accuracy                         0.8565      2000
   macro avg     0.8628    0.8567    0.8559      2000
weighted avg     0.8630    0.8565    0.8559      2000

INFO: Confusion Matrix...
INFO: [[793 210]
 [ 77 920]]
INFO: Time usage:3.161341s
