INFO: Model name '/data/tmp/zywei/competation/pretrain_models/bert-base-chinese/vocab.txt' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/data/tmp/zywei/competation/pretrain_models/bert-base-chinese/vocab.txt' is a path or url to a directory containing tokenizer files.
INFO: Didn't find file /data/tmp/zywei/competation/pretrain_models/bert-base-chinese/added_tokens.json. We won't load it.
INFO: Didn't find file /data/tmp/zywei/competation/pretrain_models/bert-base-chinese/special_tokens_map.json. We won't load it.
INFO: Didn't find file /data/tmp/zywei/competation/pretrain_models/bert-base-chinese/tokenizer_config.json. We won't load it.
INFO: loading file /data/tmp/zywei/competation/pretrain_models/bert-base-chinese/vocab.txt
INFO: loading file None
INFO: loading file None
INFO: loading file None
INFO: LOOKING AT /data/tmp/zywei/competation/try_data/train.csv
INFO: *** Example ***
INFO: guid: train-1
INFO: input_ids: 101 5687 3996 4567 4970 1366 3309 833 1139 4385 5592 3811 4568 4307 1408 102 1928 4563 5592 3811 1724 5501 3187 1213 3221 679 3221 5687 3996 4567 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: label: 0 (id = 0)
INFO: *** Example ***
INFO: guid: train-2
INFO: input_ids: 101 4507 754 5131 2228 4567 2471 6629 3314 3456 4868 5307 4142 8024 2582 720 3418 3780 8043 102 5131 2228 4567 3314 3456 4868 5307 4142 4638 3780 4545 3175 3791 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: label: 1 (id = 1)
INFO: *** Example ***
INFO: guid: train-3
INFO: input_ids: 101 150 1798 7770 6117 1327 8024 3221 6858 2792 6432 4638 7770 6117 5544 8043 102 7770 6117 1327 2471 6629 5554 1139 6117 2582 720 2843 3131 3780 4545 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: label: 0 (id = 0)
INFO: self config {
  "batch_size": 32,
  "class_list": [
    "0",
    "1"
  ],
  "config_file": "/data/tmp/zywei/competation/pretrain_models/bert-base-chinese/config.json",
  "data_dir": "/data/tmp/zywei/competation/try_data",
  "dev_num_examples": 2000,
  "dev_split": 0.1,
  "device": "cuda",
  "device_id": 3,
  "do_lower_case": true,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "is_logging2file": true,
  "label_on_test_set": true,
  "learning_rate": 2e-05,
  "logging_dir": "/data/tmp/zywei/competation/logging/base_try_data/base_bert",
  "model_name_or_path": "/data/tmp/zywei/competation/pretrain_models/bert-base-chinese/pytorch_model.bin",
  "models_name": "base_bert",
  "num_labels": 2,
  "num_train_epochs": 8,
  "pad_size": 64,
  "require_improvement": 1000,
  "requires_grad": true,
  "seed": 369,
  "task": "base_try_data",
  "test_num_examples": 2000,
  "test_split": 0.1,
  "tokenizer_file": "/data/tmp/zywei/competation/pretrain_models/bert-base-chinese/vocab.txt",
  "train_num_examples": 16000,
  "warmup_proportion": 0.1,
  "weight_decay": 0.01
}

INFO: loading configuration file /data/tmp/zywei/competation/pretrain_models/bert-base-chinese/config.json
INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": "base_try_data",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

INFO: loading weights file /data/tmp/zywei/competation/pretrain_models/bert-base-chinese/pytorch_model.bin
INFO: ***** Running training *****
INFO:   Train Num examples = 16000
INFO:   Dev Num examples = 2000
INFO:   Num Epochs = 8
INFO:   Instantaneous batch size GPU/CPU = 32
INFO:   Total optimization steps = 4000
INFO:   Train device:cuda, id:3
INFO: Epoch [1/8]
INFO: Iter:    100,  Train Loss: 0.620817,  Train Acc: 56.03%,  Val Loss: 0.552917,  Val Acc: 75.90%,  Time: 21.324267625808716 *
INFO: Iter:    200,  Train Loss: 0.325359,  Train Acc: 78.84%,  Val Loss: 0.403133,  Val Acc: 82.50%,  Time: 41.636314153671265 *
INFO: Iter:    300,  Train Loss: 0.443762,  Train Acc: 82.00%,  Val Loss: 0.372395,  Val Acc: 83.65%,  Time: 61.956990242004395 *
INFO: Iter:    400,  Train Loss: 0.382211,  Train Acc: 82.53%,  Val Loss: 0.341967,  Val Acc: 85.00%,  Time: 82.10277557373047 *
INFO: Iter:    500,  Train Loss: 0.405734,  Train Acc: 83.34%,  Val Loss: 0.344645,  Val Acc: 85.25%,  Time: 102.08166790008545 
INFO: Epoch [2/8]
INFO: Iter:    600,  Train Loss: 0.371417,  Train Acc: 87.41%,  Val Loss: 0.337219,  Val Acc: 86.85%,  Time: 122.07728981971741 *
INFO: Iter:    700,  Train Loss: 0.385490,  Train Acc: 89.22%,  Val Loss: 0.312934,  Val Acc: 86.95%,  Time: 142.07789421081543 *
INFO: Iter:    800,  Train Loss: 0.499374,  Train Acc: 88.06%,  Val Loss: 0.305817,  Val Acc: 87.60%,  Time: 162.01678085327148 *
INFO: Iter:    900,  Train Loss: 0.242990,  Train Acc: 88.19%,  Val Loss: 0.292769,  Val Acc: 88.60%,  Time: 182.0086612701416 *
INFO: Iter:   1000,  Train Loss: 0.364923,  Train Acc: 87.81%,  Val Loss: 0.301242,  Val Acc: 87.95%,  Time: 202.05354404449463 
INFO: Epoch [3/8]
INFO: Iter:   1100,  Train Loss: 0.072299,  Train Acc: 92.25%,  Val Loss: 0.317988,  Val Acc: 88.85%,  Time: 223.84149265289307 
INFO: Iter:   1200,  Train Loss: 0.354931,  Train Acc: 92.16%,  Val Loss: 0.322562,  Val Acc: 88.25%,  Time: 243.88645577430725 
INFO: Iter:   1300,  Train Loss: 0.091646,  Train Acc: 93.47%,  Val Loss: 0.326802,  Val Acc: 88.15%,  Time: 263.8989417552948 
INFO: Iter:   1400,  Train Loss: 0.249761,  Train Acc: 92.56%,  Val Loss: 0.310142,  Val Acc: 88.50%,  Time: 283.90674924850464 
INFO: Iter:   1500,  Train Loss: 0.181810,  Train Acc: 91.91%,  Val Loss: 0.304863,  Val Acc: 88.65%,  Time: 303.9448173046112 
INFO: Epoch [4/8]
INFO: Iter:   1600,  Train Loss: 0.095632,  Train Acc: 95.62%,  Val Loss: 0.363654,  Val Acc: 88.00%,  Time: 323.98440980911255 
INFO: Iter:   1700,  Train Loss: 0.049124,  Train Acc: 95.75%,  Val Loss: 0.361937,  Val Acc: 88.20%,  Time: 344.0234525203705 
INFO: Iter:   1800,  Train Loss: 0.259415,  Train Acc: 95.53%,  Val Loss: 0.396966,  Val Acc: 88.30%,  Time: 364.03938484191895 
INFO: Iter:   1900,  Train Loss: 0.168305,  Train Acc: 95.06%,  Val Loss: 0.362988,  Val Acc: 88.15%,  Time: 384.0743284225464 
INFO: No optimization for a long time, auto-stopping...
INFO: ***** Running testing *****
INFO:   Test Num examples = 2000
INFO: Test Loss: 0.3614,  Test Acc: 87.55%
INFO: Precision, Recall and F1-Score...
INFO:               precision    recall  f1-score   support

           0     0.9011    0.8445    0.8718      1003
           1     0.8528    0.9067    0.8789       997

    accuracy                         0.8755      2000
   macro avg     0.8769    0.8756    0.8754      2000
weighted avg     0.8770    0.8755    0.8754      2000

INFO: Confusion Matrix...
INFO: [[847 156]
 [ 93 904]]
INFO: Time usage:3.156774s
