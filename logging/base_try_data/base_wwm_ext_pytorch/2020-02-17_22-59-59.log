INFO: Model name '/data/tmp/zywei/competation/pretrain_models/chinese_wwm_ext_pytorch/vocab.txt' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/data/tmp/zywei/competation/pretrain_models/chinese_wwm_ext_pytorch/vocab.txt' is a path or url to a directory containing tokenizer files.
INFO: Didn't find file /data/tmp/zywei/competation/pretrain_models/chinese_wwm_ext_pytorch/added_tokens.json. We won't load it.
INFO: Didn't find file /data/tmp/zywei/competation/pretrain_models/chinese_wwm_ext_pytorch/special_tokens_map.json. We won't load it.
INFO: Didn't find file /data/tmp/zywei/competation/pretrain_models/chinese_wwm_ext_pytorch/tokenizer_config.json. We won't load it.
INFO: loading file /data/tmp/zywei/competation/pretrain_models/chinese_wwm_ext_pytorch/vocab.txt
INFO: loading file None
INFO: loading file None
INFO: loading file None
INFO: LOOKING AT /data/tmp/zywei/competation/try_data/train.csv
INFO: *** Example ***
INFO: guid: train-1
INFO: input_ids: 101 5687 3996 4567 4970 1366 3309 833 1139 4385 5592 3811 4568 4307 1408 102 1928 4563 5592 3811 1724 5501 3187 1213 3221 679 3221 5687 3996 4567 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: label: 0 (id = 0)
INFO: *** Example ***
INFO: guid: train-2
INFO: input_ids: 101 4507 754 5131 2228 4567 2471 6629 3314 3456 4868 5307 4142 8024 2582 720 3418 3780 8043 102 5131 2228 4567 3314 3456 4868 5307 4142 4638 3780 4545 3175 3791 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: label: 1 (id = 1)
INFO: *** Example ***
INFO: guid: train-3
INFO: input_ids: 101 150 1798 7770 6117 1327 8024 3221 6858 2792 6432 4638 7770 6117 5544 8043 102 7770 6117 1327 2471 6629 5554 1139 6117 2582 720 2843 3131 3780 4545 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO: label: 0 (id = 0)
INFO: self config {
  "batch_size": 32,
  "class_list": [
    "0",
    "1"
  ],
  "config_file": "/data/tmp/zywei/competation/pretrain_models/chinese_wwm_ext_pytorch/bert_config.json",
  "data_dir": "/data/tmp/zywei/competation/try_data",
  "dev_num_examples": 2000,
  "dev_split": 0.1,
  "device": "cuda",
  "device_id": 2,
  "do_lower_case": true,
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "is_logging2file": true,
  "label_on_test_set": true,
  "learning_rate": 2e-05,
  "logging_dir": "/data/tmp/zywei/competation/logging/base_try_data/base_wwm_ext_pytorch",
  "model_name_or_path": "/data/tmp/zywei/competation/pretrain_models/chinese_wwm_ext_pytorch/pytorch_model.bin",
  "models_name": "base_wwm_ext_pytorch",
  "num_labels": 2,
  "num_train_epochs": 8,
  "pad_size": 64,
  "require_improvement": 1000,
  "requires_grad": true,
  "seed": 369,
  "task": "base_try_data",
  "test_num_examples": 2000,
  "test_split": 0.1,
  "tokenizer_file": "/data/tmp/zywei/competation/pretrain_models/chinese_wwm_ext_pytorch/vocab.txt",
  "train_num_examples": 16000,
  "warmup_proportion": 0.1,
  "weight_decay": 0.01
}

INFO: loading configuration file /data/tmp/zywei/competation/pretrain_models/chinese_wwm_ext_pytorch/bert_config.json
INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "finetuning_task": "base_try_data",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 21128
}

INFO: loading weights file /data/tmp/zywei/competation/pretrain_models/chinese_wwm_ext_pytorch/pytorch_model.bin
INFO: ***** Running training *****
INFO:   Train Num examples = 16000
INFO:   Dev Num examples = 2000
INFO:   Num Epochs = 8
INFO:   Instantaneous batch size GPU/CPU = 32
INFO:   Total optimization steps = 4000
INFO:   Train device:cuda, id:2
INFO: Epoch [1/8]
INFO: Iter:    100,  Train Loss: 0.529342,  Train Acc: 60.47%,  Val Loss: 0.543865,  Val Acc: 74.30%,  Time: 20.576547145843506 *
INFO: Iter:    200,  Train Loss: 0.350808,  Train Acc: 79.03%,  Val Loss: 0.388987,  Val Acc: 82.80%,  Time: 40.73013639450073 *
INFO: Iter:    300,  Train Loss: 0.454266,  Train Acc: 82.75%,  Val Loss: 0.367072,  Val Acc: 83.30%,  Time: 61.25729727745056 *
INFO: Iter:    400,  Train Loss: 0.306663,  Train Acc: 83.59%,  Val Loss: 0.341715,  Val Acc: 85.20%,  Time: 81.59076309204102 *
INFO: Iter:    500,  Train Loss: 0.383415,  Train Acc: 84.94%,  Val Loss: 0.312811,  Val Acc: 86.85%,  Time: 101.95271944999695 *
INFO: Epoch [2/8]
INFO: Iter:    600,  Train Loss: 0.347341,  Train Acc: 87.78%,  Val Loss: 0.314098,  Val Acc: 87.05%,  Time: 122.33317947387695 
INFO: Iter:    700,  Train Loss: 0.245052,  Train Acc: 89.72%,  Val Loss: 0.323365,  Val Acc: 85.85%,  Time: 142.73206782341003 
INFO: Iter:    800,  Train Loss: 0.478554,  Train Acc: 88.50%,  Val Loss: 0.308539,  Val Acc: 87.25%,  Time: 163.1128339767456 *
INFO: Iter:    900,  Train Loss: 0.343325,  Train Acc: 87.44%,  Val Loss: 0.298760,  Val Acc: 88.20%,  Time: 183.9791488647461 *
INFO: Iter:   1000,  Train Loss: 0.304365,  Train Acc: 88.41%,  Val Loss: 0.287182,  Val Acc: 87.75%,  Time: 204.434326171875 *
INFO: Epoch [3/8]
INFO: Iter:   1100,  Train Loss: 0.076880,  Train Acc: 92.50%,  Val Loss: 0.329607,  Val Acc: 88.00%,  Time: 224.8875253200531 
INFO: Iter:   1200,  Train Loss: 0.170017,  Train Acc: 93.16%,  Val Loss: 0.315430,  Val Acc: 88.35%,  Time: 245.80223679542542 
INFO: Iter:   1300,  Train Loss: 0.117247,  Train Acc: 93.50%,  Val Loss: 0.319191,  Val Acc: 89.05%,  Time: 266.1983890533447 
INFO: Iter:   1400,  Train Loss: 0.278865,  Train Acc: 92.41%,  Val Loss: 0.321647,  Val Acc: 88.15%,  Time: 286.60014605522156 
INFO: Iter:   1500,  Train Loss: 0.221569,  Train Acc: 92.25%,  Val Loss: 0.328975,  Val Acc: 88.10%,  Time: 307.5434720516205 
INFO: Epoch [4/8]
INFO: Iter:   1600,  Train Loss: 0.051745,  Train Acc: 95.88%,  Val Loss: 0.366783,  Val Acc: 88.55%,  Time: 327.94208121299744 
INFO: Iter:   1700,  Train Loss: 0.084515,  Train Acc: 95.91%,  Val Loss: 0.364125,  Val Acc: 88.95%,  Time: 348.33504247665405 
INFO: Iter:   1800,  Train Loss: 0.367678,  Train Acc: 95.69%,  Val Loss: 0.380375,  Val Acc: 88.90%,  Time: 368.99300718307495 
INFO: Iter:   1900,  Train Loss: 0.028306,  Train Acc: 95.44%,  Val Loss: 0.359028,  Val Acc: 88.70%,  Time: 389.5004277229309 
INFO: Iter:   2000,  Train Loss: 0.297316,  Train Acc: 95.97%,  Val Loss: 0.356474,  Val Acc: 88.50%,  Time: 409.94187474250793 
INFO: Epoch [5/8]
INFO: No optimization for a long time, auto-stopping...
INFO: ***** Running testing *****
INFO:   Test Num examples = 2000
INFO: Test Loss: 0.3778,  Test Acc: 87.50%
INFO: Precision, Recall and F1-Score...
INFO:               precision    recall  f1-score   support

           0     0.9124    0.8305    0.8695      1003
           1     0.8436    0.9198    0.8800       997

    accuracy                         0.8750      2000
   macro avg     0.8780    0.8751    0.8748      2000
weighted avg     0.8781    0.8750    0.8748      2000

INFO: Confusion Matrix...
INFO: [[833 170]
 [ 80 917]]
INFO: Time usage:3.229635s
