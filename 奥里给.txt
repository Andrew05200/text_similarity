baseline:Bert常规

1.数据问题
（1）数据预处理

（2）数据增强
        A.如果提供验证集等，使用伪标签
#        B.对称性、传递性增加数据
#        C.测试数据增强：对数据中的一些词（提问词（高频词）、答案敏感词、医疗词）进行Mask，使模型更加关注语义信息。
        D.重复翻译，如CH->EN->CH
        E.使用其他比赛数据
   
   (3)  数据特征
      A.数据隐式表征
#      B.图特征（度、pagerank）
#      C.距离特征（编辑距离，汉明距离等）
#      d.N-gram特征

2.模型问题
（1）预训练模型选择
        A.RoBERTa-zh-Large
        B.RoBERTa-wwm-ext-large
#        C.BERT-wwm-ext
#       D.BERT_base(larget)

（2）模型设计
        A.端到端：
#            a.常规bert输出
#            b.使用隐藏层【CLS】加权输入
#            c.bert最后一层max-pooling和avgpooling拼接输出
            d.在bert输出后拼接特征（尝试选择）
            e.给Bert后加attention层输出（根据类别平衡程度，尝试选择）

        B.两段式
             a.bert学习文本的隐式表征
               ~ 使用Bert分别对问题编码，拼接
               ~ 对两句话一起编码
             b.使用xgboost和lightgbm对特征进行处理。

（3）训练过程
#        A.常规finetune
#        B.两段式finetune
        C.使用Multi-Sample Dropout 机制（如果存在drop层）
        D.使用相关数据做pretraing(慎选)
#        E.使用差分学习率（尝试）

 （4）模型集成
         A.stacking
         B.votting

3.可能遇到的问题
（1）类别不平衡，加权重、Focal loss
（2）threshold选择（根据输出要求）
（3）GroupKFold拆分数据(根据test情况选择)
（4）submit trick
